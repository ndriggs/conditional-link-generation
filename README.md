# conditional-link-generation
This repo is dedicated to using various approaches to generate links conditioned on invariants (you give me some invariant values, I give you a link with those invariant values). A link is one or more knots linked together. A knot is a link with one component [\[1\]](https://en.wikipedia.org/wiki/Link_(knot_theory)). Invariants are numerical, boolean, or polynomial values or "properties" associated with a knot/link that don't change with different equivalent knot diagrams (different 2D projections of the same knot/link). See [Veritasium's YouTube video](https://youtu.be/8DBhTXM_Br4?si=3nWlkOe6v9e9OonI) for a fun, accessible introduction to knot theory. 

This project was inspired by [\[2\]](https://arxiv.org/abs/2012.11293) where the authors generated molecules that optimized a certain chemical property using reinforcement learning.  

## Reinforcement Learning
Every knot and link has a braid representation. A braid is a sequence of over/under twists for a fixed number of strands. These twists create a natural set of tokens for links. In this RL environment the agent has a set of twists it can choose from to append to the braid "word" as actions. There is also an action to end the braid word. The reward can either be sparse or dense. The sparse reward is given only on episode end and is the difference between the goal invariant values and the invariant values of the final link (or some combination of the positive/negative invariant values if the goal is simply to maximize/minimize). The dense reward is given at each step and is the difference between the sparse reward that would've been given at the current timestep minus the sparse reward that would've been given at the last timestep. 

### State Representations
One key to any successful RL application is having a good state representation. The braid group has a faithful matrix representation known as the Lawrence-Krammer representation, so this seemed like a natural state representation since it's a fixed sized representation of varying-length braids and takes care of the symmetries in braids. But as it turns out, while nice mathematically, the Lawrence-Krammer representation does not behave well numerically. Though most values are on the order of $10^{-1}$ to $10^{2}$ some are on the order of $10^{5}$. In [predicting_signature](https://github.com/ndriggs/conditional-link-generation/tree/main/src/link_generation/predicting_signature/predict_signature.py) I experimented with predicting the signature of a link based on the Lawrence-Krammer representation of the associated braid. I took the abosulte value of all the values in the matrix, added 1, then took the log, and although a CNN was able to predict signature with an L1 loss of 2.29 (the signatures in the dataset ranged mostly uniformly from -37 to 37), there were other models and representations that worked better, like a GNN. The success of the GNN (L1 loss of 0.98) lead me to use the braid word representation and a feature extractor that creates a graph of the braid then extracts features from the braid using a GNN before feeding it into an MLP. The last representation I've experimented with is one-hot-encoding each twist and simplying right-padding them with zeros and feeding this representation into an MLP as in [\[3\]](https://arxiv.org/abs/1610.05744). The one representation I haven't experimented with is an RNN like they used in [\[2\]](https://arxiv.org/abs/2012.11293). 

### Hindsight Experience Replay (HER)
I did some experiments with HER, which aligns nicely with the multi-goal framework I was originally using, but it was when I was still using the Lawrence-Krammer representation. Though it showed some improvements, we have since shifted to trying to maximize signature while minimizing log determinant, which no longer falls under the multi-goal setting.  

### Curiousity 
One technique in reinforcement learning is to use intrinsic rewards (rewards generated by the agent, not the environment) to help promote exploration. One type of intrinsic reward is curiousity, where the agent has an internal network that tries to predict the next state give the current state and action. The intrinsic reward is the error between the actual next state and the predicted one. The network helps to "store" information about what state-action pairs the agent has seen so there is greater intrinsic reward when the agent explores unseen states-action pairs. Since the dynamics of this environment are known, I could adapt curiousity the same way as in [\[2\]](https://arxiv.org/abs/2012.11293) and have the network predict just the invariant values of the state, allowing the network to store information on what states have been visited. 

Extensive experiments were performed to select which architecture should be used for the curiousity network. The models considered are found in [curiousity_models](https://github.com/ndriggs/conditional-link-generation/blob/main/src/link_generation/models/curiousity_models.py). I have still yet to implement curiousity, but the experiments helped me learn a lot about the expressiveness of these different models and discover the power of GNNs. 

### Monte Carlo Tree Search
Since we know the transition dynamics, monte carlo tree search seems like a natural approach and is something I hope to try in the future. [LightZero](https://github.com/opendilab/LightZero) seems like a good tool to try. 

## Variational Autoencoder (VAE)
See [link_generation/vae](https://github.com/ndriggs/conditional-link-generation/tree/main/src/link_generation/vae). I create a VAE for knots represented by potholder diagrams whose reconstruction loss is based on the invariant values of the original knot and reconstructed knot, similar to perceptual loss for VAEs on images. However, our invariant calculation is an exact calculation based on the Goeritz matrix, not using a frozen neural net like in perceptual loss. 

## Other Potential Approaches
Simulated annealing, genetic algorithm, discrete bayesian optimization, tree-structured parzen estimator, fine-tuning an LLM with the invariant values of a knot followed by its braid word representation. 
